{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Going Deeper - Loading Models with Code\n",
    "## Understanding What's Under the Hood\n",
    "\n",
    "**Today's Goals:**\n",
    "1. Understand the difference between `pipeline` (easy) and manual loading (more control)\n",
    "2. Learn about tokenizers - how models \"read\" text\n",
    "3. Load and configure models manually\n",
    "4. Understand model outputs in detail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pipeline vs Manual Loading\n",
    "\n",
    "Last week we used `pipeline()` - it's like using a microwave:\n",
    "- Put food in, press button, get hot food\n",
    "- Easy, but limited control\n",
    "\n",
    "Today we'll learn to \"cook from scratch\":\n",
    "- More steps, but you control everything\n",
    "- Understand what's really happening\n",
    "\n",
    "**When to use which:**\n",
    "- **Pipeline**: Quick experiments, standard tasks\n",
    "- **Manual**: Custom behavior, learning how it works, fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install torch -q\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: What is a Tokenizer?\n",
    "\n",
    "AI models don't read words - they read **numbers**.\n",
    "\n",
    "A **tokenizer** converts:\n",
    "- Text â†’ Numbers (for the model to read)\n",
    "- Numbers â†’ Text (for us to read the output)\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize some text\n",
    "text = \"Hello, I am learning about AI!\"\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"\\nTokenized (numbers): {tokens['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what each number represents\n",
    "print(\"What each token means:\")\n",
    "for token_id in tokens['input_ids']:\n",
    "    word = tokenizer.decode([token_id])\n",
    "    print(f\"  {token_id} â†’ '{word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens:\n",
    "\n",
    "Notice some special tokens:\n",
    "- `[CLS]` (101) - Start of sequence\n",
    "- `[SEP]` (102) - End of sequence\n",
    "- `[PAD]` (0) - Padding (for equal lengths)\n",
    "\n",
    "These help the model understand where sentences begin and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different words - see how they're tokenized\n",
    "words = [\"hello\", \"Hello\", \"HELLO\", \"artificial\", \"AI\", \"ðŸ¤–\"]\n",
    "\n",
    "print(\"How different words get tokenized:\")\n",
    "for word in words:\n",
    "    tokens = tokenizer(word)['input_ids']\n",
    "    # Remove special tokens for clarity\n",
    "    tokens = tokens[1:-1]  \n",
    "    print(f\"  '{word}' â†’ {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight:\n",
    "\n",
    "Some words get split into multiple tokens! This is called **subword tokenization**.\n",
    "- \"artificial\" might become [\"art\", \"##ificial\"]\n",
    "- This helps the model handle words it hasn't seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Loading a Model Manually\n",
    "\n",
    "Now let's load the actual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load a sentiment analysis model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model Step by Step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the text\n",
    "text = \"I love learning about AI!\"\n",
    "\n",
    "# Step 2: Tokenize (convert to numbers)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # pt = PyTorch tensors\n",
    "print(\"Step 2 - Tokenized input:\")\n",
    "print(f\"  input_ids: {inputs['input_ids']}\")\n",
    "print(f\"  attention_mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run through the model\n",
    "with torch.no_grad():  # We're not training, just predicting\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"Step 3 - Raw model output:\")\n",
    "print(f\"  logits: {outputs.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert to probabilities\n",
    "probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "print(\"Step 4 - Probabilities:\")\n",
    "print(f\"  NEGATIVE: {probabilities[0][0]:.2%}\")\n",
    "print(f\"  POSITIVE: {probabilities[0][1]:.2%}\")\n",
    "\n",
    "# Step 5: Get the prediction\n",
    "prediction = torch.argmax(probabilities).item()\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "print(f\"\\nFinal prediction: {labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Process:\n",
    "\n",
    "```\n",
    "Text â†’ Tokenizer â†’ Numbers â†’ Model â†’ Logits â†’ Softmax â†’ Probabilities â†’ Prediction\n",
    "```\n",
    "\n",
    "**Pipeline does all this automatically!** But now you understand what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Let's Make a Function\n",
    "\n",
    "Let's wrap this in a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, show_details=False):\n",
    "    \"\"\"Predict sentiment of text with optional details.\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = torch.softmax(outputs.logits, dim=1)[0]\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_idx = torch.argmax(probs).item()\n",
    "    labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Tokens: {len(inputs['input_ids'][0])} tokens\")\n",
    "        print(f\"NEGATIVE prob: {probs[0]:.2%}\")\n",
    "        print(f\"POSITIVE prob: {probs[1]:.2%}\")\n",
    "        print(f\"Prediction: {labels[pred_idx]}\")\n",
    "        print()\n",
    "    \n",
    "    return labels[pred_idx], probs[pred_idx].item()\n",
    "\n",
    "# Test it\n",
    "predict_sentiment(\"This is amazing!\", show_details=True)\n",
    "predict_sentiment(\"I hate waiting in line.\", show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: The Auto Classes\n",
    "\n",
    "Hugging Face has \"Auto\" classes that pick the right model type automatically:\n",
    "\n",
    "| Auto Class | Use For |\n",
    "|------------|--------|\n",
    "| `AutoTokenizer` | Any tokenizer |\n",
    "| `AutoModel` | Base model (just embeddings) |\n",
    "| `AutoModelForSequenceClassification` | Text classification |\n",
    "| `AutoModelForQuestionAnswering` | Q&A tasks |\n",
    "| `AutoModelForCausalLM` | Text generation |\n",
    "| `AutoModelForSeq2SeqLM` | Translation, summarization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a Q&A model manually\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "qa_model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "\n",
    "print(f\"Loaded Q&A model: {qa_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Q&A model\n",
    "context = \"The Eiffel Tower is located in Paris, France. It was built in 1889.\"\n",
    "question = \"Where is the Eiffel Tower?\"\n",
    "\n",
    "# Tokenize both question and context\n",
    "inputs = qa_tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get answer\n",
    "with torch.no_grad():\n",
    "    outputs = qa_model(**inputs)\n",
    "\n",
    "# Find the answer span\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Decode the answer\n",
    "answer_tokens = inputs['input_ids'][0][answer_start:answer_end]\n",
    "answer = qa_tokenizer.decode(answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Comparing Pipeline vs Manual\n",
    "\n",
    "Let's compare the two approaches side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline approach (easy)\n",
    "print(\"=== PIPELINE APPROACH ===\")\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this!\")\n",
    "print(f\"Result: {result}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual approach (more control)\n",
    "print(\"=== MANUAL APPROACH ===\")\n",
    "label, confidence = predict_sentiment(\"I love this!\", show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each:\n",
    "\n",
    "**Use Pipeline when:**\n",
    "- Quick prototyping\n",
    "- Standard tasks\n",
    "- You just need the answer\n",
    "\n",
    "**Use Manual when:**\n",
    "- You need custom processing\n",
    "- You want to understand the internals\n",
    "- You're preparing for fine-tuning\n",
    "- You need specific outputs (embeddings, attention, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge: Build Your Own Classifier Function\n",
    "\n",
    "Create a function that:\n",
    "1. Takes a list of texts\n",
    "2. Returns predictions for all of them\n",
    "3. Shows a summary at the end\n",
    "\n",
    "**Hint:** Ask AI to help you build on the `predict_sentiment` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your challenge code here!\n",
    "# Try asking AI: \"Modify this function to handle a list of texts and show statistics\"\n",
    "\n",
    "def analyze_many_texts(texts):\n",
    "    \"\"\"Analyze multiple texts and show summary.\"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Best purchase ever!\",\n",
    "    \"Waste of money.\"\n",
    "]\n",
    "\n",
    "# analyze_many_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### The Loading Pattern:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelFor[Task]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model-name\")\n",
    "model = AutoModelFor[Task].from_pretrained(\"model-name\")\n",
    "```\n",
    "\n",
    "### The Prediction Pattern:\n",
    "```python\n",
    "# 1. Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 2. Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 3. Process outputs\n",
    "probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checklist: What You Learned Today\n",
    "\n",
    "- [ ] What a tokenizer does (text â†” numbers)\n",
    "- [ ] How to load models manually with Auto classes\n",
    "- [ ] The full prediction flow: tokenize â†’ model â†’ softmax â†’ prediction\n",
    "- [ ] When to use pipeline vs manual loading\n",
    "- [ ] How to wrap model code in reusable functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead: Next Week\n",
    "\n",
    "Next week we'll explore **image models**:\n",
    "- Image classification (what's in this picture?)\n",
    "- How vision models \"see\" images\n",
    "- Loading and using image AI\n",
    "\n",
    "**Homework (optional):**\n",
    "- Complete the challenge above\n",
    "- Try loading a different model type manually\n",
    "- Save your work to GitHub!\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
