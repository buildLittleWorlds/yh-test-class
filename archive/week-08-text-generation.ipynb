{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Text Generation Models\n",
    "## How ChatGPT-Style AI Works\n",
    "\n",
    "**Today's Goals:**\n",
    "1. Understand how text generation models work\n",
    "2. Use small GPT models that run on free Colab\n",
    "3. Control generation with parameters (temperature, length, etc.)\n",
    "4. Explore what these models can and can't do\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How Text Generation Works\n",
    "\n",
    "Text generation models predict **one word at a time**.\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "Model thinks: What word comes next?\n",
    "  - \"mat\" (30% likely)\n",
    "  - \"floor\" (25% likely)\n",
    "  - \"couch\" (20% likely)\n",
    "  - etc.\n",
    "Output: \"mat\"\n",
    "\n",
    "Then it continues:\n",
    "Input: \"The cat sat on the mat\"\n",
    "Output: \"and\"\n",
    "...\n",
    "```\n",
    "\n",
    "This is called **autoregressive generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers -q\n",
    "!pip install torch -q\n",
    "\n",
    "print(\"Libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Your First Text Generator\n",
    "\n",
    "We'll use **DistilGPT-2** - a smaller, faster version of GPT-2 that works great on free Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a text generation model\n",
    "# distilgpt2 is 82M parameters - small and fast!\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "print(\"Text generator loaded! (DistilGPT-2 - 82M parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text!\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "result = generator(prompt, max_length=50)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGenerated:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Different Prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try these prompts\n",
    "prompts = [\n",
    "    \"The robot walked into the room and\",\n",
    "    \"In the year 2050, humans will\",\n",
    "    \"The best advice I ever received was\",\n",
    "    \"Scientists discovered that\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    result = generator(prompt, max_length=60, num_return_sequences=1)\n",
    "    print(f\"\\nGenerated: {result[0]['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Controlling Generation\n",
    "\n",
    "You can control how the model generates text using parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature: Creativity Control\n",
    "\n",
    "- **Low temperature (0.1-0.5)**: More predictable, \"safer\" outputs\n",
    "- **High temperature (0.8-1.5)**: More creative, random outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The meaning of life is\"\n",
    "\n",
    "print(\"Comparing temperatures:\\n\")\n",
    "\n",
    "# Low temperature - predictable\n",
    "print(\"--- Temperature = 0.3 (predictable) ---\")\n",
    "result = generator(prompt, max_length=40, temperature=0.3, do_sample=True)\n",
    "print(result[0]['generated_text'])\n",
    "print()\n",
    "\n",
    "# High temperature - creative\n",
    "print(\"--- Temperature = 1.2 (creative) ---\")\n",
    "result = generator(prompt, max_length=40, temperature=1.2, do_sample=True)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Length: How Much Text to Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "\n",
    "print(\"Comparing lengths:\\n\")\n",
    "\n",
    "for length in [20, 50, 100]:\n",
    "    print(f\"--- max_length = {length} ---\")\n",
    "    result = generator(prompt, max_length=length)\n",
    "    print(result[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Outputs: Get Different Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"My favorite hobby is\"\n",
    "\n",
    "# Generate 3 different completions\n",
    "results = generator(\n",
    "    prompt, \n",
    "    max_length=40, \n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Version {i}: {result['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: The Bigger Model - GPT-2\n",
    "\n",
    "Let's try the full GPT-2 small model (124M parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original GPT-2 (small)\n",
    "gpt2_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "print(\"GPT-2 loaded! (124M parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DistilGPT-2 vs GPT-2\n",
    "prompt = \"Artificial intelligence will change the world by\"\n",
    "\n",
    "print(\"Comparing models on the same prompt:\\n\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "print(\"--- DistilGPT-2 (82M params) ---\")\n",
    "result1 = generator(prompt, max_length=60)\n",
    "print(result1[0]['generated_text'])\n",
    "print()\n",
    "\n",
    "print(\"--- GPT-2 (124M params) ---\")\n",
    "result2 = gpt2_generator(prompt, max_length=60)\n",
    "print(result2[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Important Limitations!\n",
    "\n",
    "These small models have limitations you should know:\n",
    "\n",
    "### What They CAN Do:\n",
    "- Complete sentences\n",
    "- Generate creative text\n",
    "- Follow patterns in the prompt\n",
    "\n",
    "### What They CAN'T Do Well:\n",
    "- Answer questions accurately\n",
    "- Follow complex instructions\n",
    "- Remember context from far back\n",
    "- Reason about facts\n",
    "\n",
    "**ChatGPT is MUCH bigger** (175 billion+ parameters vs our 82-124 million!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the limitations - asking a factual question\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "result = generator(prompt, max_length=30)\n",
    "print(\"Trying a factual prompt:\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "# Sometimes it gets it right, sometimes not!\n",
    "# This is why bigger models like ChatGPT are more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Creative Writing Fun!\n",
    "\n",
    "These models are great for creative writing experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_story(opening, length=150, temperature=0.8):\n",
    "    \"\"\"Generate a story from an opening line.\"\"\"\n",
    "    result = generator(\n",
    "        opening,\n",
    "        max_length=length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Generate a story!\n",
    "story = write_story(\"It was a dark and stormy night when the mysterious stranger arrived at\")\n",
    "print(\"Generated Story:\")\n",
    "print(\"=\" * 50)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different genres\n",
    "genres = {\n",
    "    \"Sci-Fi\": \"The spaceship landed on the alien planet, and the astronauts saw\",\n",
    "    \"Mystery\": \"Detective Smith examined the clue carefully. It was\",\n",
    "    \"Romance\": \"She looked into his eyes and realized that\",\n",
    "    \"Horror\": \"The old house creaked as something moved in the darkness. It was\"\n",
    "}\n",
    "\n",
    "for genre, opening in genres.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    story = write_story(opening, length=80)\n",
    "    print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Understanding Why AI Says Weird Things\n",
    "\n",
    "Sometimes AI generates strange or nonsensical text. Why?\n",
    "\n",
    "1. **It's just predicting the next word** - no real understanding\n",
    "2. **Training data patterns** - it learned from internet text\n",
    "3. **Randomness** - high temperature means more random choices\n",
    "4. **No fact-checking** - it doesn't know what's true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the model generate some questionable content\n",
    "# (because it's just pattern matching, not reasoning)\n",
    "\n",
    "prompt = \"The moon is made of\"\n",
    "\n",
    "print(\"Let's see what the model thinks the moon is made of:\\n\")\n",
    "for i in range(3):\n",
    "    result = generator(prompt, max_length=30, do_sample=True, temperature=1.0)\n",
    "    print(f\"{i+1}. {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Build a Text Generator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_writer(prompt, style=\"normal\"):\n",
    "    \"\"\"\n",
    "    Generate text with different styles.\n",
    "    \n",
    "    Styles:\n",
    "    - 'conservative': Safe, predictable\n",
    "    - 'normal': Balanced\n",
    "    - 'creative': Wild and random\n",
    "    \"\"\"\n",
    "    settings = {\n",
    "        \"conservative\": {\"temperature\": 0.3, \"max_length\": 60},\n",
    "        \"normal\": {\"temperature\": 0.7, \"max_length\": 80},\n",
    "        \"creative\": {\"temperature\": 1.2, \"max_length\": 100}\n",
    "    }\n",
    "    \n",
    "    s = settings[style]\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=s[\"max_length\"],\n",
    "        temperature=s[\"temperature\"],\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Style: {style.upper()}\")\n",
    "    print(f\"Temperature: {s['temperature']}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[0]['generated_text'])\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Try it!\n",
    "test_prompt = \"The secret to happiness is\"\n",
    "\n",
    "for style in [\"conservative\", \"normal\", \"creative\"]:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    creative_writer(test_prompt, style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parameter Reference\n",
    "\n",
    "| Parameter | What it does | Typical values |\n",
    "|-----------|-------------|----------------|\n",
    "| `max_length` | Max tokens to generate | 50-200 |\n",
    "| `temperature` | Creativity (higher = more random) | 0.3-1.2 |\n",
    "| `do_sample` | Enable randomness | True/False |\n",
    "| `num_return_sequences` | How many outputs | 1-5 |\n",
    "| `top_k` | Consider only top K words | 10-50 |\n",
    "| `top_p` | Consider words in top P probability | 0.9-0.95 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checklist: What You Learned Today\n",
    "\n",
    "- [ ] How text generation works (next word prediction)\n",
    "- [ ] Using DistilGPT-2 and GPT-2 models\n",
    "- [ ] Controlling output with temperature and other parameters\n",
    "- [ ] Why AI sometimes says weird or wrong things\n",
    "- [ ] The difference between small and large language models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead: Next Week\n",
    "\n",
    "Next week we'll explore **datasets**:\n",
    "- Finding datasets on Hugging Face\n",
    "- Loading and exploring data\n",
    "- Understanding how datasets are used for training\n",
    "\n",
    "**Homework (optional):**\n",
    "- Generate stories with different prompts\n",
    "- Experiment with temperature settings\n",
    "- Save your favorite generations to GitHub!\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
