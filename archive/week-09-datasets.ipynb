{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Datasets on Hugging Face\n",
    "## The Fuel for AI Models\n",
    "\n",
    "**Today's Goals:**\n",
    "1. Understand why datasets matter for AI\n",
    "2. Navigate the Hugging Face Datasets Hub\n",
    "3. Load and explore datasets in Python\n",
    "4. Prepare for fine-tuning (next steps!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Datasets Matter\n",
    "\n",
    "**AI models learn from data.**\n",
    "\n",
    "Think of it like this:\n",
    "- A **model** is like a student\n",
    "- A **dataset** is like a textbook\n",
    "- **Training** is like studying\n",
    "\n",
    "The quality and type of data determines what the model learns!\n",
    "\n",
    "```\n",
    "Dataset (examples) → Training → Model (learned patterns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Examples:\n",
    "\n",
    "| Dataset | What model learns |\n",
    "|---------|------------------|\n",
    "| Movie reviews (positive/negative) | Sentiment analysis |\n",
    "| Images of cats and dogs | Image classification |\n",
    "| Question-answer pairs | Question answering |\n",
    "| English-French sentence pairs | Translation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the datasets library\n",
    "!pip install datasets -q\n",
    "!pip install pandas -q\n",
    "\n",
    "print(\"Libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Ready to explore datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Hugging Face Datasets Hub\n",
    "\n",
    "**Browse datasets at:** [huggingface.co/datasets](https://huggingface.co/datasets)\n",
    "\n",
    "There are 100,000+ datasets covering:\n",
    "- Text (sentiment, Q&A, translation)\n",
    "- Images (classification, object detection)\n",
    "- Audio (speech recognition, music)\n",
    "- And more!\n",
    "\n",
    "### How to Find Datasets:\n",
    "1. Use the **Task** filter (what do you want to do?)\n",
    "2. Sort by **Downloads** (popular = reliable)\n",
    "3. Check the **Size** (smaller = faster to load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Loading Your First Dataset\n",
    "\n",
    "Let's load the famous **IMDB dataset** - 50,000 movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the structure\n",
    "print(\"Dataset structure:\")\n",
    "print(f\"  Training examples: {len(imdb['train'])}\")\n",
    "print(f\"  Test examples: {len(imdb['test'])}\")\n",
    "print(f\"\\nColumns: {imdb['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "example = imdb['train'][0]\n",
    "\n",
    "print(\"First training example:\")\n",
    "print(f\"\\nLabel: {example['label']} ({'Positive' if example['label'] == 1 else 'Negative'})\")\n",
    "print(f\"\\nReview (first 500 chars):\")\n",
    "print(example['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Labels:\n",
    "- `0` = Negative review\n",
    "- `1` = Positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few more examples\n",
    "print(\"Sample reviews:\\n\")\n",
    "for i in range(3):\n",
    "    ex = imdb['train'][i]\n",
    "    sentiment = \"POSITIVE\" if ex['label'] == 1 else \"NEGATIVE\"\n",
    "    print(f\"--- Example {i+1} ({sentiment}) ---\")\n",
    "    print(ex['text'][:200] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Exploring Other Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Dataset\n",
    "\n",
    "Tweets labeled with 6 emotions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion dataset\n",
    "emotion = load_dataset(\"emotion\")\n",
    "\n",
    "print(\"Emotion dataset loaded!\")\n",
    "print(f\"Training examples: {len(emotion['train'])}\")\n",
    "print(f\"Columns: {emotion['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The emotion labels\n",
    "emotion_labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Look at examples of each emotion\n",
    "print(\"Examples of each emotion:\\n\")\n",
    "for i, label in enumerate(emotion_labels):\n",
    "    # Find an example with this label\n",
    "    for ex in emotion['train']:\n",
    "        if ex['label'] == i:\n",
    "            print(f\"{label.upper()}: \\\"{ex['text']}\\\"\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatoes Dataset\n",
    "\n",
    "A smaller sentiment dataset - good for quick experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Rotten Tomatoes (smaller, faster)\n",
    "rotten = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "print(\"Rotten Tomatoes dataset:\")\n",
    "print(f\"  Training: {len(rotten['train'])} examples\")\n",
    "print(f\"  Validation: {len(rotten['validation'])} examples\")\n",
    "print(f\"  Test: {len(rotten['test'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sizes\n",
    "print(\"Dataset size comparison:\\n\")\n",
    "print(f\"IMDB: {len(imdb['train']):,} training examples\")\n",
    "print(f\"Rotten Tomatoes: {len(rotten['train']):,} training examples\")\n",
    "print(f\"Emotion: {len(emotion['train']):,} training examples\")\n",
    "\n",
    "print(\"\\nSmaller datasets are faster to train on!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Working with Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Specific Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific examples\n",
    "first_10 = imdb['train'][:10]\n",
    "print(f\"First 10 examples: {len(first_10['text'])} items\")\n",
    "\n",
    "# Get a range\n",
    "middle = imdb['train'][100:105]\n",
    "print(f\"Examples 100-104: {len(middle['text'])} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only positive reviews\n",
    "positive_reviews = imdb['train'].filter(lambda x: x['label'] == 1)\n",
    "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
    "\n",
    "# Filter to only negative reviews\n",
    "negative_reviews = imdb['train'].filter(lambda x: x['label'] == 0)\n",
    "print(f\"Negative reviews: {len(negative_reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Pandas (for easier exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(rotten['train'])\n",
    "\n",
    "print(\"As a pandas DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nAverage text length:\")\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(f\"  {df['text_length'].mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Train/Test Split - Why It Matters\n",
    "\n",
    "Most datasets have splits:\n",
    "\n",
    "- **Train**: Data the model learns from\n",
    "- **Validation**: Data to check progress during training\n",
    "- **Test**: Data to evaluate final performance\n",
    "\n",
    "**Important:** The test data should NEVER be seen during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the splits\n",
    "print(\"IMDB splits:\")\n",
    "for split in imdb.keys():\n",
    "    print(f\"  {split}: {len(imdb[split])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample for quick experiments\n",
    "small_train = imdb['train'].shuffle(seed=42).select(range(1000))\n",
    "small_test = imdb['test'].shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(f\"Small training set: {len(small_train)} examples\")\n",
    "print(f\"Small test set: {len(small_test)} examples\")\n",
    "print(\"\\nSmaller datasets = faster experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: What Makes a Good Dataset?\n",
    "\n",
    "When choosing or creating a dataset, consider:\n",
    "\n",
    "### 1. Size\n",
    "- More data usually = better model\n",
    "- But smaller datasets are faster to train\n",
    "\n",
    "### 2. Quality\n",
    "- Accurate labels matter!\n",
    "- Clean, consistent data\n",
    "\n",
    "### 3. Balance\n",
    "- Equal examples of each category\n",
    "- Imbalanced data can bias the model\n",
    "\n",
    "### 4. Relevance\n",
    "- Does it match your actual use case?\n",
    "- Training on tweets won't help with legal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if IMDB is balanced\n",
    "train_df = pd.DataFrame(imdb['train'])\n",
    "\n",
    "print(\"IMDB label balance:\")\n",
    "counts = train_df['label'].value_counts()\n",
    "print(f\"  Negative (0): {counts[0]} ({counts[0]/len(train_df):.1%})\")\n",
    "print(f\"  Positive (1): {counts[1]} ({counts[1]/len(train_df):.1%})\")\n",
    "print(\"\\nPerfectly balanced! (This is good)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Challenge - Explore a New Dataset\n",
    "\n",
    "Your turn! Find and explore a dataset that interests you.\n",
    "\n",
    "**Ideas:**\n",
    "- `ag_news` - News articles (4 categories)\n",
    "- `squad` - Question-answering\n",
    "- `tweet_eval` - Tweet classification\n",
    "- `yelp_review_full` - 5-star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading a new dataset!\n",
    "# Uncomment and modify:\n",
    "\n",
    "# my_dataset = load_dataset(\"ag_news\")\n",
    "# print(my_dataset)\n",
    "# print(my_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset_name):\n",
    "    \"\"\"Helper function to explore any dataset.\"\"\"\n",
    "    print(f\"Loading {dataset_name}...\")\n",
    "    ds = load_dataset(dataset_name)\n",
    "    \n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Splits\n",
    "    print(\"\\nSplits:\")\n",
    "    for split in ds.keys():\n",
    "        print(f\"  {split}: {len(ds[split])} examples\")\n",
    "    \n",
    "    # Columns\n",
    "    first_split = list(ds.keys())[0]\n",
    "    print(f\"\\nColumns: {ds[first_split].column_names}\")\n",
    "    \n",
    "    # Sample\n",
    "    print(f\"\\nFirst example:\")\n",
    "    for key, value in ds[first_split][0].items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Try it!\n",
    "# explore_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset(\"dataset_name\")\n",
    "\n",
    "# Access splits\n",
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "\n",
    "# Get examples\n",
    "example = train[0]           # First example\n",
    "batch = train[:10]           # First 10\n",
    "shuffled = train.shuffle()   # Random order\n",
    "\n",
    "# Filter\n",
    "filtered = train.filter(lambda x: x['label'] == 1)\n",
    "\n",
    "# Convert to pandas\n",
    "df = pd.DataFrame(train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checklist: What You Learned Today\n",
    "\n",
    "- [ ] Why datasets are essential for AI (data → training → model)\n",
    "- [ ] How to browse the Hugging Face Datasets Hub\n",
    "- [ ] Loading datasets with `load_dataset()`\n",
    "- [ ] Exploring dataset structure and examples\n",
    "- [ ] Train/test splits and why they matter\n",
    "- [ ] What makes a good dataset (size, quality, balance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead: Next Week\n",
    "\n",
    "Next week we'll **plan our portfolio projects**:\n",
    "- Brainstorm project ideas\n",
    "- Choose a dataset to work with\n",
    "- Plan the fine-tuning approach\n",
    "\n",
    "**Homework (optional):**\n",
    "- Explore 2-3 datasets that interest you\n",
    "- Think about what you'd like to build\n",
    "- Save your exploration to GitHub!\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
