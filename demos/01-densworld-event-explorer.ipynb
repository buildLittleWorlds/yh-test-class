{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densworld Event Explorer\n",
    "\n",
    "**A demonstration of Hugging Face pipelines applied to rich narrative data**\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load structured event data (JSONL format)\n",
    "2. Apply **zero-shot classification** to categorize events\n",
    "3. Use **question answering** to explore event relationships\n",
    "\n",
    "---\n",
    "\n",
    "## About Densworld\n",
    "\n",
    "Densworld is a fictional universe with 1,189 logged events spanning centuries. Events include:\n",
    "- Expeditions and explorations\n",
    "- Boundary anomalies and spatial phenomena\n",
    "- Scholarly theories and manuscripts\n",
    "- Political changes and institutional responses\n",
    "\n",
    "The Living Ledger tracks causal chains: events trigger consequences that become new events.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Demo?\n",
    "\n",
    "This notebook demonstrates Level 2 concepts with **real data**:\n",
    "- Zero-shot classification (no training required)\n",
    "- Question-answering pipelines\n",
    "- Working with structured narrative data\n",
    "\n",
    "You can adapt this approach for your own datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required libraries and upload the event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not already installed (Colab may have it)\n",
    "!pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# For Colab: upload files\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"Running in Colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload EVENT_LOG.jsonl (in Colab)\n",
    "# If running locally, place the file in the same directory\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Please upload EVENT_LOG.jsonl when prompted...\")\n",
    "    uploaded = files.upload()\n",
    "    filename = list(uploaded.keys())[0]\n",
    "else:\n",
    "    filename = \"EVENT_LOG.jsonl\"\n",
    "\n",
    "print(f\"Using file: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Event Data\n",
    "\n",
    "The Living Ledger is stored in JSONL format - one JSON object per line. Each event has:\n",
    "- `event_id`: Unique identifier (e.g., \"EV-847-001\")\n",
    "- `type`: Event category (e.g., \"boundary_anomaly\", \"theory_proposed\")\n",
    "- `date`: When it happened in Densworld time\n",
    "- `location`: Where it happened\n",
    "- `actors`: Characters involved\n",
    "- `notes`: Narrative description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all events from JSONL\n",
    "events = []\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            events.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(events)} events\")\n",
    "print(f\"\\nFirst event:\")\n",
    "print(json.dumps(events[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(events)\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"Event Types:\")\n",
    "print(df['type'].value_counts().head(15))\n",
    "print(f\"\\nTotal unique event types: {df['type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some interesting events with notes\n",
    "events_with_notes = df[df['notes'].notna()].copy()\n",
    "print(f\"Events with narrative notes: {len(events_with_notes)}\")\n",
    "print(\"\\nSample event notes:\")\n",
    "for _, row in events_with_notes.head(5).iterrows():\n",
    "    print(f\"\\n[{row['event_id']}] {row['type']}\")\n",
    "    print(f\"  {row['notes'][:200]}...\" if len(str(row['notes'])) > 200 else f\"  {row['notes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zero-Shot Classification\n",
    "\n",
    "**Zero-shot classification** lets us categorize text without training a custom model. We provide:\n",
    "- Text to classify\n",
    "- A list of possible labels\n",
    "\n",
    "The model determines which label best fits the text.\n",
    "\n",
    "### Use Case: Urgency Classification\n",
    "\n",
    "Let's classify events by **urgency level** - something not in the original data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load zero-shot classification pipeline\n",
    "# This uses facebook/bart-large-mnli by default\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "print(\"Zero-shot classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define urgency labels\n",
    "urgency_labels = [\n",
    "    \"routine observation\",\n",
    "    \"notable discovery\",\n",
    "    \"urgent situation\",\n",
    "    \"crisis or emergency\"\n",
    "]\n",
    "\n",
    "# Test on a few events\n",
    "sample_events = events_with_notes.head(5)\n",
    "\n",
    "print(\"Classifying events by urgency...\\n\")\n",
    "for _, row in sample_events.iterrows():\n",
    "    text = row['notes']\n",
    "    result = classifier(text, urgency_labels)\n",
    "    \n",
    "    print(f\"[{row['event_id']}] {row['type']}\")\n",
    "    print(f\"  Text: {text[:100]}...\")\n",
    "    print(f\"  Urgency: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also classify by THEME\n",
    "theme_labels = [\n",
    "    \"scientific investigation\",\n",
    "    \"political or institutional\",\n",
    "    \"personal or character-driven\",\n",
    "    \"mysterious or unexplained\",\n",
    "    \"conflict or danger\"\n",
    "]\n",
    "\n",
    "print(\"Classifying events by theme...\\n\")\n",
    "for _, row in sample_events.iterrows():\n",
    "    text = row['notes']\n",
    "    result = classifier(text, theme_labels)\n",
    "    \n",
    "    print(f\"[{row['event_id']}] {row['type']}\")\n",
    "    print(f\"  Theme: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "    print(f\"  Runner-up: {result['labels'][1]} ({result['scores'][1]:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create Your Own Categories\n",
    "\n",
    "Try classifying events with your own labels! Ideas:\n",
    "- `[\"success\", \"failure\", \"ambiguous outcome\"]`\n",
    "- `[\"individual action\", \"group action\", \"natural phenomenon\"]`\n",
    "- `[\"reversible\", \"permanent change\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Define custom labels and classify events\n",
    "my_labels = [\"your\", \"labels\", \"here\"]\n",
    "\n",
    "# Pick an event\n",
    "test_event = events_with_notes.iloc[10]\n",
    "print(f\"Event: {test_event['notes']}\")\n",
    "\n",
    "# Classify it\n",
    "# result = classifier(test_event['notes'], my_labels)\n",
    "# print(f\"Classification: {result['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "**Extractive QA** finds answers within a given context. We provide:\n",
    "- A **question**\n",
    "- A **context** (text that contains the answer)\n",
    "\n",
    "The model extracts the answer from the context.\n",
    "\n",
    "### Use Case: Ask Questions About Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "print(\"QA pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context from multiple events about the SW collapse\n",
    "sw_events = df[df['notes'].str.contains('SW|southwest|collapse', case=False, na=False)]\n",
    "context = \" \".join(sw_events['notes'].dropna().tolist()[:10])\n",
    "\n",
    "print(f\"Context length: {len(context)} characters\")\n",
    "print(f\"\\nContext preview: {context[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the events\n",
    "questions = [\n",
    "    \"What was the breathing phenomenon?\",\n",
    "    \"Who predicted the collapse?\",\n",
    "    \"What happened to the moat?\",\n",
    "    \"Who was the chief surveyor?\"\n",
    "]\n",
    "\n",
    "print(\"Asking questions about SW collapse events...\\n\")\n",
    "for q in questions:\n",
    "    result = qa_pipeline(question=q, context=context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['score']:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with a different context - scholarly theories\n",
    "theory_events = df[df['type'] == 'theory_proposed']\n",
    "theory_context = \" \".join(theory_events['notes'].dropna().tolist()[:10])\n",
    "\n",
    "theory_questions = [\n",
    "    \"What is the Library hypothesis?\",\n",
    "    \"What did Keth propose?\",\n",
    "    \"What causes landmarks to appear in different positions?\"\n",
    "]\n",
    "\n",
    "print(\"Asking questions about theories...\\n\")\n",
    "for q in theory_questions:\n",
    "    result = qa_pipeline(question=q, context=theory_context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['score']:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Ask Your Own Questions\n",
    "\n",
    "Create a context from events and ask questions about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Create a context and ask questions\n",
    "\n",
    "# Filter events (example: events about disappearances)\n",
    "# my_events = df[df['type'] == 'disappearance']\n",
    "# my_context = \" \".join(my_events['notes'].dropna().tolist())\n",
    "\n",
    "# Ask a question\n",
    "# result = qa_pipeline(question=\"Who disappeared?\", context=my_context)\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combining Both: Event Analysis Tool\n",
    "\n",
    "Let's build a simple tool that:\n",
    "1. Takes an event ID\n",
    "2. Shows the event details\n",
    "3. Classifies its urgency and theme\n",
    "4. Finds related events and answers questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_event(event_id):\n",
    "    \"\"\"Analyze a Densworld event using ML pipelines.\"\"\"\n",
    "    \n",
    "    # Find the event\n",
    "    event = df[df['event_id'] == event_id]\n",
    "    if len(event) == 0:\n",
    "        print(f\"Event {event_id} not found!\")\n",
    "        return\n",
    "    \n",
    "    event = event.iloc[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"EVENT ANALYSIS: {event_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nType: {event['type']}\")\n",
    "    print(f\"Date: {event['date']}\")\n",
    "    print(f\"Location: {event['location']}\")\n",
    "    if event.get('actors'):\n",
    "        print(f\"Actors: {', '.join(event['actors'])}\")\n",
    "    \n",
    "    # Notes\n",
    "    if pd.notna(event.get('notes')):\n",
    "        print(f\"\\nNotes: {event['notes']}\")\n",
    "        \n",
    "        # Classify urgency\n",
    "        urgency = classifier(event['notes'], urgency_labels)\n",
    "        print(f\"\\nUrgency: {urgency['labels'][0]} ({urgency['scores'][0]:.2%})\")\n",
    "        \n",
    "        # Classify theme\n",
    "        theme = classifier(event['notes'], theme_labels)\n",
    "        print(f\"Theme: {theme['labels'][0]} ({theme['scores'][0]:.2%})\")\n",
    "    \n",
    "    # Find related events (same type or location)\n",
    "    related = df[\n",
    "        ((df['type'] == event['type']) | (df['location'] == event['location'])) &\n",
    "        (df['event_id'] != event_id)\n",
    "    ].head(3)\n",
    "    \n",
    "    if len(related) > 0:\n",
    "        print(f\"\\nRelated Events:\")\n",
    "        for _, r in related.iterrows():\n",
    "            print(f\"  - {r['event_id']}: {r['type']} at {r['location']}\")\n",
    "    \n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the analysis tool!\n",
    "analyze_event(\"EV-847-002\")  # The SW-6 breathing phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another event\n",
    "analyze_event(\"EV-895-001\")  # The Library hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Load JSONL data** - A common format for structured text data\n",
    "2. **Zero-shot classification** - Categorize text without training\n",
    "3. **Question answering** - Extract answers from context\n",
    "4. **Combine pipelines** - Build analysis tools\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Zero-shot classification** is powerful for adding new metadata to existing data\n",
    "- **QA pipelines** work best when you provide focused, relevant context\n",
    "- **Hugging Face pipelines** make it easy to experiment quickly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try classifying ALL events and visualize the distribution\n",
    "2. Build a character tracker that follows one actor across events\n",
    "3. Create a timeline visualization of classified events\n",
    "4. Experiment with different classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Show distribution of event types\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "type_counts = df['type'].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "type_counts.plot(kind='barh')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Event Type')\n",
    "plt.title('Top 10 Event Types in Densworld')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
